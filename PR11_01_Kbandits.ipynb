{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX0qqi1zGq0fAdMfOfm0W0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Carlos-Villasenor/PatternRecognition/blob/main/PR11_01_Kbandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgPHJXYPa2GP"
      },
      "source": [
        "#Reconocimiento de Patrones\n",
        "##Dr. Carlos Villase침or\n",
        "##Introducci칩n al aprendizaje reforzado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENTuhwNFH-42"
      },
      "source": [
        "# k-armed Bandit Testbed\n",
        "# Dr. Carlos Villase침or\n",
        "# 30/08/2018\n",
        "\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Bandit class\n",
        "class Bandit:\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.act_vals = np.random.randn(n)\n",
        "\n",
        "    def take_action(self, action):\n",
        "\n",
        "        # Catch action errors\n",
        "        if (action < 0) or (action >= self.n) or (type(action) != int):\n",
        "            raise ValueError('Action not recognized')\n",
        "\n",
        "        # Return reward with some noise\n",
        "        return np.random.randn() + self.act_vals[action]\n",
        "\n",
        "    def show_action_values(self):\n",
        "        return self.act_vals\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xx3K9u-Gs4C"
      },
      "source": [
        "# Epsilon-greedy Algoritm\n",
        "class epsilon_greedy:\n",
        "\n",
        "    def __init__(self, num_actions, epsilon):\n",
        "\n",
        "        # Number of actions\n",
        "        self.num_act = num_actions\n",
        "\n",
        "        # Probability of exploration\n",
        "        self.eps = epsilon\n",
        "\n",
        "        # Estimation of action-values\n",
        "        self.Q = np.zeros(num_actions)\n",
        "\n",
        "        # Action selection counter\n",
        "        self.n = np.zeros(num_actions)\n",
        "\n",
        "\n",
        "    def learn(self, reward_function, iterations=1000):\n",
        "\n",
        "        self.rewards = np.zeros(iterations)\n",
        "\n",
        "        # Iterations\n",
        "        for i in range(iterations):\n",
        "\n",
        "            # Choose with Epsilon-Greedy strategy\n",
        "            rand_num = np.random.random()\n",
        "            if rand_num < self.eps:\n",
        "                action = int(np.random.randint(self.num_act))\n",
        "            else:\n",
        "                action = int(np.argmax(self.Q))\n",
        "\n",
        "            # Get reward of the action\n",
        "            reward = reward_function(action)\n",
        "            self.rewards[i] = reward\n",
        "\n",
        "            # Update estimates using the Sample-Average Incremental method\n",
        "            self.n[action] += 1\n",
        "            self.Q[action] += (1/self.n[action])*(reward - self.Q[action])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monte Carlo Simulation -----------------------------------------------------\n",
        "plt.close('all')\n",
        "\n",
        "# Number of arms\n",
        "n = 10\n",
        "\n",
        "# Number of iterations\n",
        "iterations = 1000\n",
        "\n",
        "# Number of  Monte-Carlo Simulations\n",
        "MCtest = 2000\n",
        "\n",
        "# array for saving reward record\n",
        "hist_1 = np.zeros(iterations)\n",
        "hist_2 = np.zeros(iterations)\n",
        "hist_3 = np.zeros(iterations)\n",
        "hist_4 = np.zeros(iterations)\n",
        "\n",
        "for i in range(MCtest):\n",
        "\n",
        "    # Create k-armed Bandit\n",
        "    B = Bandit(n)\n",
        "\n",
        "    # Create Agent with epsilon-greedy strategy\n",
        "    Agent_1 = epsilon_greedy(n, 0.1)\n",
        "    Agent_2 = epsilon_greedy(n, 0.01)\n",
        "    Agent_3 = epsilon_greedy(n, 0.)\n",
        "    Agent_4 = epsilon_greedy(n, 0.5)\n",
        "\n",
        "    # Learn policy\n",
        "    Agent_1.learn(B.take_action, iterations)\n",
        "    Agent_2.learn(B.take_action, iterations)\n",
        "    Agent_3.learn(B.take_action, iterations)\n",
        "    Agent_4.learn(B.take_action, iterations)\n",
        "\n",
        "    # Aggregate reward record\n",
        "    hist_1  += Agent_1.rewards\n",
        "    hist_2  += Agent_2.rewards\n",
        "    hist_3  += Agent_3.rewards\n",
        "    hist_4  += Agent_4.rewards\n",
        "\n",
        "# Calculate mean reward record\n",
        "hist_1 /= MCtest\n",
        "hist_2 /= MCtest\n",
        "hist_3 /= MCtest\n",
        "hist_4 /= MCtest\n",
        "\n",
        "# Plot result\n",
        "plt.plot(hist_1 ,'-r', linewidth=0.5, label=r'$\\epsilon$-Greedy $\\epsilon=0.1$')\n",
        "plt.plot(hist_2 ,'-b', linewidth=0.5, label=r'$\\epsilon$-Greedy $\\epsilon=0.01$')\n",
        "plt.plot(hist_3 ,'-g', linewidth=0.5, label='Greedy')\n",
        "plt.plot(hist_4 ,'-y', linewidth=0.5, label=r'$\\epsilon$-Greedy $\\epsilon=0.5$')\n",
        "plt.legend()\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title(r'$\\epsilon$-Greedy comparison')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VBzp3vwTIZgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD5aIf0pG3Hl"
      },
      "source": [
        "# Optimistic Initial Values\n",
        "# Dr. Carlos Villase침or\n",
        "# 31/08/2018\n",
        "# From the book of RL 2.6\n",
        "\n",
        "# Epsilon-Greedy algorithm with constant alpha\n",
        "class epsilon_greedy:\n",
        "\n",
        "    def __init__(self, num_actions, epsilon):\n",
        "\n",
        "        # Number of actions\n",
        "        self.num_act = num_actions\n",
        "\n",
        "        # Probability of exploration\n",
        "        self.eps = epsilon\n",
        "\n",
        "        # Estimation of values\n",
        "        self.Q = np.zeros(num_actions)\n",
        "\n",
        "    def learn(self, reward_function, iterations=1000, alpha=0.1):\n",
        "\n",
        "        self.rewards = np.zeros(iterations)\n",
        "\n",
        "        # Iterations\n",
        "        for i in range(iterations):\n",
        "\n",
        "            # Choose with Epsilon-Greedy strategy\n",
        "            rand_num = np.random.random()\n",
        "            if rand_num < self.eps:\n",
        "                action = int(np.random.randint(self.num_act))\n",
        "            else:\n",
        "                action = int(np.argmax(self.Q))\n",
        "\n",
        "            # Get reward of the action\n",
        "            reward = reward_function(action)\n",
        "            self.rewards[i] = reward\n",
        "\n",
        "            # Update estimates using the Sample-Average Incremental method\n",
        "            self.Q[action] += alpha * (reward - self.Q[action])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monte Carlo Simulation -----------------------------------------------------\n",
        "plt.close('all')\n",
        "\n",
        "# Number of arms\n",
        "n = 10\n",
        "\n",
        "# Number of iterations\n",
        "iterations = 1000\n",
        "\n",
        "# Number of  Monte-Carlo Simulations\n",
        "MCtest = 2000\n",
        "\n",
        "# array for saving reward record\n",
        "hist_1 = np.zeros(iterations)\n",
        "hist_2 = np.zeros(iterations)\n",
        "\n",
        "for i in range(MCtest):\n",
        "\n",
        "    # Create k-armed Bandit\n",
        "    B = Bandit(n)\n",
        "\n",
        "    # Create Greedy Optimistic Agent\n",
        "    Agent_1 = epsilon_greedy(n, 0)\n",
        "    Agent_1.Q = np.full(n, 5.0)\n",
        "\n",
        "    # Create normal eps-greedy Agent\n",
        "    Agent_2 = epsilon_greedy(n, 0.1)\n",
        "\n",
        "    # Learn policy\n",
        "    Agent_1.learn(B.take_action, iterations)\n",
        "    Agent_2.learn(B.take_action, iterations)\n",
        "\n",
        "    # Aggregate reward record\n",
        "    hist_1  += Agent_1.rewards\n",
        "    hist_2  += Agent_2.rewards\n",
        "\n",
        "# Calculate mean reward record\n",
        "hist_1 /= MCtest\n",
        "hist_2 /= MCtest\n",
        "\n",
        "# Plot result\n",
        "plt.plot(hist_1 ,'-r', linewidth=0.5, label='Optimistic Greedy')\n",
        "plt.plot(hist_2 ,'-b', linewidth=0.5, label=r'$\\epsilon$-Greedy $\\epsilon=0.1$')\n",
        "plt.legend()\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title(r'Optimistic Greedy vs $\\epsilon$-Greedy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wKLpDJFQIupT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhU6Cba_HPFL"
      },
      "source": [
        "# Upper-Confidence-Bound vs Epsilon-Greedy\n",
        "# Dr. Carlos Villase침or\n",
        "# 31/08/2018\n",
        "# From the book of RL 2.7\n",
        "\n",
        "# Value-estimator Class\n",
        "class Sample_Average_Estimator:\n",
        "    def __init__(self, num_actions, initial_estimate):\n",
        "\n",
        "        # Action-value estimations\n",
        "        self.Q = np.full(num_actions, float(initial_estimate))\n",
        "\n",
        "        # Times the action has been taken\n",
        "        self.N = np.zeros(num_actions)\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # Incremental version of the Sample Average\n",
        "        self.N[action] += 1\n",
        "        self.Q[action] += (1/self.N[action])*(reward - self.Q[action])\n",
        "\n",
        "    def argmax(self):\n",
        "        return int(np.argmax(self.Q))\n",
        "\n",
        "class UCB:\n",
        "\n",
        "    def __init__(self, num_actions, confidence):\n",
        "\n",
        "        # Number of actions\n",
        "        self.num_act = num_actions\n",
        "\n",
        "        # Confidence\n",
        "        self.c = confidence\n",
        "\n",
        "        # Estimation of values\n",
        "        self.Q = np.zeros(num_actions)\n",
        "\n",
        "        # Estimator\n",
        "        self.SAE = Sample_Average_Estimator(num_actions, 0.0)\n",
        "\n",
        "    def learn(self, reward_function, iterations=1000):\n",
        "\n",
        "        self.rewards = np.zeros(iterations)\n",
        "\n",
        "        # Iterations\n",
        "        for i in range(1, iterations+1):\n",
        "\n",
        "            # Calculate UCB\n",
        "            best = -np.inf\n",
        "            for j in range(self.num_act):\n",
        "                if self.SAE.N[j] == 0:\n",
        "                    action = j\n",
        "                    break\n",
        "                else:\n",
        "                    ucb = self.SAE.Q[j] + self.c * np.sqrt(np.log(i)/self.SAE.N[j])\n",
        "                    if ucb > best:\n",
        "                        action = j\n",
        "                        best = ucb\n",
        "\n",
        "            # Get reward of the action\n",
        "            reward = reward_function(action)\n",
        "            self.rewards[i-1] = reward\n",
        "\n",
        "            # Update estimates\n",
        "            self.SAE.update(action, reward)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monte Carlo Simulation -----------------------------------------------------\n",
        "\n",
        "plt.close('all')\n",
        "\n",
        "# Number of arms\n",
        "n = 10\n",
        "\n",
        "# Number of iterations\n",
        "iterations = 1000\n",
        "\n",
        "# Number of  Monte-Carlo Simulations\n",
        "MCtest = 2000\n",
        "\n",
        "# array for saving reward record\n",
        "hist_1 = np.zeros(iterations)\n",
        "hist_2 = np.zeros(iterations)\n",
        "\n",
        "for i in range(MCtest):\n",
        "    if i%(MCtest/10) == 0:\n",
        "        print('Complete: ', i/(MCtest/100), '%')\n",
        "\n",
        "    # Create k-armed Bandit\n",
        "    B = Bandit(n)\n",
        "\n",
        "    # Create epsilon-greedy Agent\n",
        "    Agent_1 = epsilon_greedy(n, epsilon=0.1)\n",
        "\n",
        "    # Create UCB Agent\n",
        "    Agent_2 = UCB(n, confidence=2)\n",
        "\n",
        "    # Learn policy\n",
        "    Agent_1.learn(B.take_action, iterations)\n",
        "    Agent_2.learn(B.take_action, iterations)\n",
        "\n",
        "    # Aggregate reward record\n",
        "    hist_1  += Agent_1.rewards\n",
        "    hist_2  += Agent_2.rewards\n",
        "\n",
        "# Calculate mean reward record\n",
        "hist_1 /= MCtest\n",
        "hist_2 /= MCtest\n",
        "\n",
        "# Plot result\n",
        "plt.plot(hist_1 ,'-r', linewidth=0.5, label=r'$\\epsilon$-Greedy $\\epsilon=0.1$')\n",
        "plt.plot(hist_2 ,'-b', linewidth=0.5, label='UCB c=2')\n",
        "plt.legend()\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title(r'UCB vs $\\epsilon$-Greedy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5B0Lb96uJk3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Bandits"
      ],
      "metadata": {
        "id": "QVF06BHia0xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(H):\n",
        "    h = H - np.max(H)\n",
        "    exp = np.exp(h)\n",
        "    return exp / np.sum(exp)\n",
        "\n",
        "class GradientBandit:\n",
        "\n",
        "    def __init__(self, num_actions, alpha=0.1):\n",
        "\n",
        "        # Number of actions\n",
        "        self.num_act = num_actions\n",
        "\n",
        "        # Confidence\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # Estimation of values\n",
        "        self.Q = np.zeros(num_actions)\n",
        "\n",
        "        # Estimator\n",
        "        self.H = np.zeros(num_actions)\n",
        "\n",
        "    def learn(self, reward_function, iterations=1000):\n",
        "\n",
        "        self.rewards = np.zeros(iterations)\n",
        "\n",
        "        # Iterations\n",
        "        for i in range(1, iterations+1):\n",
        "\n",
        "            policy = softmax(self.H)\n",
        "            action = np.random.choice(self.num_act, p=policy)\n",
        "\n",
        "            reward = reward_function(action)\n",
        "            self.rewards[i-1] = reward\n",
        "\n",
        "            avg_reward = np.average(self.rewards)\n",
        "\n",
        "            # Update H for a == A_t\n",
        "            self.H[action] = self.H[action] + self.alpha*(reward-avg_reward)*(1-policy[action])\n",
        "\n",
        "            # Update H for a != A_t\n",
        "            self.H[:action] = self.H[:action] - self.alpha*(reward-avg_reward)*policy[:action]\n",
        "            self.H[action+1:] = self.H[action+1:] - self.alpha*(reward-avg_reward)*policy[action+1:]"
      ],
      "metadata": {
        "id": "7-X_3U_5a4gC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monte Carlo Simulation -----------------------------------------------------\n",
        "\n",
        "plt.close('all')\n",
        "\n",
        "# Number of arms\n",
        "n = 10\n",
        "\n",
        "# Number of iterations\n",
        "iterations = 1000\n",
        "\n",
        "# Number of  Monte-Carlo Simulations\n",
        "MCtest = 1000\n",
        "\n",
        "# array for saving reward record\n",
        "hist_1 = np.zeros(iterations)\n",
        "hist_2 = np.zeros(iterations)\n",
        "\n",
        "for i in range(MCtest):\n",
        "    if i%(MCtest/10) == 0:\n",
        "        print('Complete: ', i/(MCtest/100), '%')\n",
        "\n",
        "    # Create k-armed Bandit\n",
        "    B = Bandit(n)\n",
        "\n",
        "    # Create UCB agent\n",
        "    Agent_1 = UCB(n, confidence=2)\n",
        "\n",
        "    # Create Gradient Bandit\n",
        "    Agent_2 = GradientBandit(n, alpha=0.1)\n",
        "\n",
        "    # Learn policy\n",
        "    Agent_1.learn(B.take_action, iterations)\n",
        "    Agent_2.learn(B.take_action, iterations)\n",
        "\n",
        "    # Aggregate reward record\n",
        "    hist_1  += Agent_1.rewards\n",
        "    hist_2  += Agent_2.rewards\n",
        "\n",
        "# Calculate mean reward record\n",
        "hist_1 /= MCtest\n",
        "hist_2 /= MCtest\n",
        "\n",
        "# Plot result\n",
        "plt.plot(hist_1 ,'-r', linewidth=0.5, label=r'UCB $c=2$')\n",
        "plt.plot(hist_2 ,'-b', linewidth=0.5, label=r'GB $\\alpha=0.1$')\n",
        "plt.legend()\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.title(r'Gradient Bandit vs UCB')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rJYrSBzfdL4_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}