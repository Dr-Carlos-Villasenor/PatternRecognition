{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6SSgMZpXZt9Y8Pz7KUlIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Carlos-Villasenor/PatternRecognition/blob/main/PR02_02_regresion_polinomial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocimeinto de Patrones\n",
        "## Dr. Carlos Vilaseñor\n",
        "## Regresión Polinomial"
      ],
      "metadata": {
        "id": "uhSFgLciP6CB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importamo paquetes"
      ],
      "metadata": {
        "id": "6z3Hh_YTxNTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Páquetes básicos\n",
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Páquetes de sklearn\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "JRYxjZIRxm7F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Creamos datos de manera sintética"
      ],
      "metadata": {
        "id": "Kd6cL1MbxocG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Semilla del generador pseudoaleatorio\n",
        "np.random.seed(42)\n",
        "\n",
        "# Número de muestras\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        "\n",
        "# Dibujar datos\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BJFwiYFDxnpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Las características polinómicas (también llamado expanción polinomial) es un pretratamiento que podemos hacerle a los datos. Se generan nuevos datos a partir de combinaciones polinomiales de los datos de entrada. Por ejemplo, suponga que tenemos una entrada $(x_1, x_2)$ podemos expandirlo con las caracteristicas de grado 2 a $(1, x_1, x_2, x_1^2, x_2^2, x_1x_2)$.\n",
        "\n",
        "En el sigueiten código hacemos la expansión de grado 3 de los datos:"
      ],
      "metadata": {
        "id": "K58KAaBGxvHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar objeto (no incluimos el bías que sería el termino independiente)\n",
        "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
        "\n",
        "# Crear nuevos datos\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# Comaperemos los datos\n",
        "print(X[0])\n",
        "print(X_poly[0])"
      ],
      "metadata": {
        "id": "0xpYEbP0xzHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Ahora bien siempre antes de introducir los datos a cualquier técnica de Aprendizaje Automático, se recomienda escalarlos (algunas técnicas es necesario y en otras no, ante la duda siempre noramalizar ya que no suele afectar a las técnicas que no lo necesitan)"
      ],
      "metadata": {
        "id": "8QqreEMfx2tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intanciamos escalador\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Entrenar y escalar\n",
        "X_poly_scaled = scaler.fit_transform(X_poly)"
      ],
      "metadata": {
        "id": "CKZyG7aqx7EA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Al entrenar un regresor lineal sobre estas nuevas entradas en realidad estámos haciendo la regresión sobre los coheficientes de un polinomio:\n",
        "\n",
        "$$\\hat{y} = \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_0$$"
      ],
      "metadata": {
        "id": "FLlXKXIrx-SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intanciar regresor\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "# Entrenar regresor\n",
        "lin_reg.fit(X_poly_scaled, y)"
      ],
      "metadata": {
        "id": "R5B-RJJvyDGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Dibujemos la linea de regresión"
      ],
      "metadata": {
        "id": "sU3QYgMvyHTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una serie de datos de -3 a 3 interpolados linealmente\n",
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "\n",
        "# Expandemos los puntos con caracteristicas polinomicas\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "\n",
        "# Normalizamos los datos\n",
        "X_new_poly_scaled = scaler.transform(X_new_poly)\n",
        "\n",
        "# Hacemos la predicción de los datos\n",
        "y_new = lin_reg.predict(X_new_poly_scaled)\n",
        "\n",
        "# Dibujamos los datos originales\n",
        "plt.plot(X, y, \"b.\")\n",
        "\n",
        "# Dibujamos la linea de regresión\n",
        "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "\n",
        "# Agregamos etiquetas\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A2QzzzrNyKbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lin_reg.predict(scaler.transform(poly_features.transform([[1.5]]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu6rH8rnyOYO",
        "outputId": "fcd14090-e359-40bd-fe57-1cbc1ac9740a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.53230346]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Para evitar lo anterior vamos a encapsular los modelos con la herramienta Pipeline"
      ],
      "metadata": {
        "id": "Mrn5JldcySuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare el grado de la regresión polinomial\n",
        "degree = 3\n",
        "\n",
        "# Instanciemos los submodelos\n",
        "polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "std_scaler = StandardScaler()\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "# Crear modelo integrado por los diferentes submodelos\n",
        "model = Pipeline([(\"poly_features\", polybig_features),\n",
        "                  (\"std_scaler\", std_scaler),\n",
        "                  (\"lin_reg\", lin_reg)])"
      ],
      "metadata": {
        "id": "COEFjeM_yWXg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Ahora podemos simplemente usar este modelo para entrenar y predecir."
      ],
      "metadata": {
        "id": "eHn32Xa6ya1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar modelo\n",
        "model.fit(X, y)\n",
        "\n",
        "# Calculamos el desempeño del modelo (r2_score)\n",
        "print('r2-score: ', model.score(X, y))\n",
        "\n",
        "# Dibujar salida\n",
        "y_new = model.predict(X_new)\n",
        "plt.plot(X_new, y_new, 'r--', label=str(degree), linewidth=2)\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "05NV1vkVycY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Repite los códigos anteriores con los grados siguientes: 1, 2, 300, (el alumno deberá guardar estás imagenes con su reporte). Reflexione en porque el modelo con grado 100 muestra el mejor desempeño. A su parecer cual ee el mejor grado para este problema."
      ],
      "metadata": {
        "id": "k5sLPi0fyqg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demostrando generalización\n",
        "\n",
        "El objetivo de entrenar un modelo de aprendizaje automático es aprender de los datos los patrones de importancia, ignorando el ruido que los datos puedan contener. Cuando el modelo es muy sencillo (grado=1) no le es posible aprender la tendencia de los datos (decimos que se subentrena). Por otra parte, cuando el modelo es muy complejo (grado=300) aprendemeos incluso el ruido de los datos (decimos que está sobreentrenado). Cuando la complejidad del modelo es acorde al patrón que tenemos que reconocer decimos que generaliza bien.\n",
        "\n",
        "Como se mostró en el experimento anterior, no podemos saber si un modelo está subentrenado o sobreentrenado con solo una medida de desempeño por lo que necesitamos al menos 2. Existen varios esquemas para demostrar generalización, el más clásico consiste en particionar de manera aleatoria los datos dejando un porcentaje mayoritario para entrenar y el menor para probar. Comparando los desempeños con datos de entrenamiento y prueba podemos diagnosticar el aprendizaje del modelo.\n",
        "\n",
        "A continuación particionamos los datos:"
      ],
      "metadata": {
        "id": "nkRujWhmyzoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Particionamos los datos 75% para entrenar y 25% para probar, estos porcentajes dependen de la cantidad de datos\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "# Imprimimos los tamaños de cada conjunto\n",
        "print(\"Entrada original: \", X.shape)\n",
        "print(\"Entrada entrenamiento: \", xtrain.shape)\n",
        "print(\"Entrada prueba: \", xtest.shape)\n",
        "\n",
        "print(\"Salida original: \", y.shape)\n",
        "print(\"Salida entrenamiento: \", ytrain.shape)\n",
        "print(\"Salida prueba: \", ytest.shape)"
      ],
      "metadata": {
        "id": "qaUf4hYOy6lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare el grado de la regresión polinomial\n",
        "degree = 300\n",
        "\n",
        "# Instanciemos los submodelos\n",
        "polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "std_scaler = StandardScaler()\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "# Crear modelo integrado por los diferentes submodelos\n",
        "model = Pipeline([(\"poly_features\", polybig_features),\n",
        "                  (\"std_scaler\", std_scaler),\n",
        "                  (\"lin_reg\", lin_reg)])\n",
        "\n",
        "# Entrenar modelo solo con los datos de entrenamiento\n",
        "model.fit(xtrain, ytrain)\n",
        "\n",
        "# Calculamos el desempeño del modelo (r2_score)\n",
        "print('Train: ', model.score(xtrain, ytrain))\n",
        "print('Test: ', model.score(xtest, ytest))\n",
        "\n",
        "# Dibujar salida\n",
        "y_new = model.predict(X_new)\n",
        "plt.plot(X_new, y_new, 'k--', label='modelo', linewidth=2)\n",
        "plt.plot(xtrain, ytrain, \"b.\", label='train', linewidth=3)\n",
        "plt.plot(xtest, ytest, \"r.\",label='test', linewidth=3)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$y$\")\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tsMJrGDiy9Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusión\n",
        "\n",
        "Para medir un problema de aprendizaje es necesario particionar en un conjunto de entrenamiento y prueba. El modelo se entrena solo con el conjunto de entrenamiento pero se calcula el desempeño para ambos conjuntos. Se puede diagnosticar el modelo de acuerdo a los siguiente\n",
        "\n",
        "+ Train-score alto, Test-score bajo: Sobreentrenamiento\n",
        "+ Train-score bajo, Test-score bajo: Subentrenamiento\n",
        "+ Train-score alto, Test-score alto: Bien generalizado\n",
        "\n",
        "Sacando todas las combinaciones, también podriamos considerar el caso en que el Train-score sea bajo y el Test-score es alto, este caso debería ser imposible en caso de encontrarse solo indicaría que el conjunto de entrenamiento y prueba tienes distribuciones diferentes."
      ],
      "metadata": {
        "id": "vSDoYMT2zG-Q"
      }
    }
  ]
}